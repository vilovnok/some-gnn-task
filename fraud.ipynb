{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f72db76-312e-4a67-9cd2-c8838a3306d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch_scatter/_version_cuda.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24b8ea5-9e87-4b17-99dd-aa58ba501801",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fafa21c1-a58c-4020-8d43-4ab73fba1291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if True:\n",
    "#   !gdown --id 1CIFpAquzYBA98gQCdMb92fC0w6yrYf2m\n",
    "#   !gdown --id 1Cfh0VIXWTc8EK96WRZdyaqgA2-JwvsUG\n",
    "#   !gdown --id 1Cfh8hA9Tl8uCPrLSmcIQI3qCbEjOFl7C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581cd09e-1980-4e54-a71e-be22c7fdfdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mv elliptic_txs_classes.csv dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57418fe7-b1f1-4d7d-b2f1-096074b16f87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from glob import glob \n",
    "dataset = glob('./dataset/*')\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a27f012-b6db-48ae-a074-b44692f68480",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = pd.read_csv(dataset[0])\n",
    "df_edges = pd.read_csv(dataset[1])\n",
    "df_features = pd.read_csv(dataset[2], header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbbe28d4-e5d8-47d5-bf96-6bf92429cad7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203769, 2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc6cf128-8006-45d1-aaa0-801792cdbb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes['class'] = df_classes['class'].map({'unknown': 2, '1': 1, '2': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8c2310b8-7cf7-421d-a388-791f54498de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BarContainer object of 3 artists>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "group_class = df_classes.groupby('class').count()\n",
    "plt.title(\"# of nodes per class\")\n",
    "plt.barh([ 'Licit','Illicit', 'Unknown'], group_class['txId'].values, color=['g', 'orange', 'r'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40dd8ca7-7334-41e2-b1bd-f9d41f04058c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>txId</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1076</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.168500</td>\n",
       "      <td>0.270909</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163591</td>\n",
       "      <td>...</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.470019</td>\n",
       "      <td>1.216796</td>\n",
       "      <td>1.151607</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "      <td>1076</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2534</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.170834</td>\n",
       "      <td>-0.131425</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>0.055376</td>\n",
       "      <td>0.054722</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.955101</td>\n",
       "      <td>0.459257</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.099080</td>\n",
       "      <td>-0.122137</td>\n",
       "      <td>-0.379970</td>\n",
       "      <td>-0.379288</td>\n",
       "      <td>2534</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3181</td>\n",
       "      <td>34</td>\n",
       "      <td>1.305212</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>97.300650</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>1.348765</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>1.969527</td>\n",
       "      <td>0.037532</td>\n",
       "      <td>-0.131010</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.017772</td>\n",
       "      <td>3181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3321</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>...</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.140597</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "      <td>3321</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3889</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.086232</td>\n",
       "      <td>-0.101835</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>17.046997</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.074885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.082065</td>\n",
       "      <td>0.114773</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>8.948005</td>\n",
       "      <td>1.024948</td>\n",
       "      <td>-0.009570</td>\n",
       "      <td>-0.080708</td>\n",
       "      <td>-0.123601</td>\n",
       "      <td>3889</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 169 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0   1         2         3         4         5          6         7  \\\n",
       "0  1076  48 -0.168500  0.270909 -0.091383 -0.046932  -0.043875 -0.029140   \n",
       "1  2534   6 -0.170834 -0.131425  1.018602  0.028105   0.055376  0.054722   \n",
       "2  3181  34  1.305212 -0.210553 -1.756361 -0.121970  97.300650 -0.113002   \n",
       "3  3321   1 -0.169615 -0.184668 -1.201369 -0.121970  -0.043875 -0.113002   \n",
       "4  3889  48 -0.086232 -0.101835 -0.646376 -0.121970  17.046997 -0.113002   \n",
       "\n",
       "          8         9  ...       159       160       161       162       163  \\\n",
       "0 -0.061584 -0.163591  ...  1.461330  1.461369  0.018279  0.470019  1.216796   \n",
       "1 -0.061584 -0.163572  ...  0.955101  0.459257 -0.098889 -0.087490 -0.099080   \n",
       "2 -0.061584  1.348765  ...  0.059948  0.113967 -0.098889  1.969527  0.037532   \n",
       "3 -0.061584 -0.160199  ...  0.241128  0.241406 -0.098889 -0.087490 -0.084674   \n",
       "4 -0.061584 -0.074885  ...  0.082065  0.114773 -0.098889  8.948005  1.024948   \n",
       "\n",
       "        164       165       166  txId  class  \n",
       "0  1.151607  1.519700  1.521399  1076      2  \n",
       "1 -0.122137 -0.379970 -0.379288  2534      0  \n",
       "2 -0.131010  0.006994  0.017772  3181      0  \n",
       "3 -0.140597  1.519700  1.521399  3321      2  \n",
       "4 -0.009570 -0.080708 -0.123601  3889      2  \n",
       "\n",
       "[5 rows x 169 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge data\n",
    "\n",
    "df_merge = df_features.merge(df_classes, how='left', right_on='txId', left_on=0)\n",
    "df_merge = df_merge.sort_values(0).reset_index(drop=True)\n",
    "df_merge.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "02900cc6-4096-423b-81e0-d12ae8b9224a",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_merge['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c95013c1-557d-491c-82a8-aa3c1e86c6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge Inde: Создадим взаимо связь между узлами\n",
    "nodes = df_merge[0].values\n",
    "node2idx = {j:i for i,j in enumerate(nodes)}\n",
    "\n",
    "edges = df_edges.copy()\n",
    "edges.txId1 = edges.txId1.map(node2idx)\n",
    "edges.txId2 = edges.txId2.map(node2idx)\n",
    "edges = edges.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ed0b2b3-b9c1-4fbc-b077-577220cf99df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[138670, 141325, 139232,  ..., 100420,  54833, 101159],\n",
       "        [  4142, 142201, 139223,  ..., 100419,  81951, 101163]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_index = np.array(edges.values).T\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).contiguous()\n",
    "\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21a002bd-c10b-47d0-812d-bda00e787813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем веса для каждой ноды\n",
    "weights = torch.ones(edge_index.shape[1], dtype=torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba5f19ab-6de4-4f41-b7ed-bc1e6623dde8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1076</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.168500</td>\n",
       "      <td>0.270909</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163591</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.470019</td>\n",
       "      <td>1.216796</td>\n",
       "      <td>1.151607</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2534</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.170834</td>\n",
       "      <td>-0.131425</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>0.055376</td>\n",
       "      <td>0.054722</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.379357</td>\n",
       "      <td>0.955101</td>\n",
       "      <td>0.459257</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.099080</td>\n",
       "      <td>-0.122137</td>\n",
       "      <td>-0.379970</td>\n",
       "      <td>-0.379288</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3181</td>\n",
       "      <td>34</td>\n",
       "      <td>1.305212</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>97.300650</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>1.348765</td>\n",
       "      <td>...</td>\n",
       "      <td>1.590664</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>1.969527</td>\n",
       "      <td>0.037532</td>\n",
       "      <td>-0.131010</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.017772</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3321</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.500080</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.140597</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3889</td>\n",
       "      <td>48</td>\n",
       "      <td>-0.086232</td>\n",
       "      <td>-0.101835</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>17.046997</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.074885</td>\n",
       "      <td>...</td>\n",
       "      <td>0.362510</td>\n",
       "      <td>0.082065</td>\n",
       "      <td>0.114773</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>8.948005</td>\n",
       "      <td>1.024948</td>\n",
       "      <td>-0.009570</td>\n",
       "      <td>-0.080708</td>\n",
       "      <td>-0.123601</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203764</th>\n",
       "      <td>403203785</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.172978</td>\n",
       "      <td>-0.172527</td>\n",
       "      <td>0.463609</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163640</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.600999</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203765</th>\n",
       "      <td>403234712</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.172669</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626229</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.216057</td>\n",
       "      <td>-0.125939</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.269818</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203766</th>\n",
       "      <td>403234715</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.172669</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626229</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.216057</td>\n",
       "      <td>-0.125939</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.269818</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203767</th>\n",
       "      <td>403235564</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.172669</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.063725</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163323</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.626229</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.216057</td>\n",
       "      <td>-0.125939</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.269818</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203768</th>\n",
       "      <td>403244581</td>\n",
       "      <td>28</td>\n",
       "      <td>-0.143292</td>\n",
       "      <td>-0.158783</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.133266</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.613614</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.131155</td>\n",
       "      <td>-0.097524</td>\n",
       "      <td>-0.120613</td>\n",
       "      <td>-0.119792</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203769 rows × 168 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0   1         2         3         4         5          6  \\\n",
       "0            1076  48 -0.168500  0.270909 -0.091383 -0.046932  -0.043875   \n",
       "1            2534   6 -0.170834 -0.131425  1.018602  0.028105   0.055376   \n",
       "2            3181  34  1.305212 -0.210553 -1.756361 -0.121970  97.300650   \n",
       "3            3321   1 -0.169615 -0.184668 -1.201369 -0.121970  -0.043875   \n",
       "4            3889  48 -0.086232 -0.101835 -0.646376 -0.121970  17.046997   \n",
       "...           ...  ..       ...       ...       ...       ...        ...   \n",
       "203764  403203785  28 -0.172978 -0.172527  0.463609 -0.121970  -0.043875   \n",
       "203765  403234712  28 -0.172669 -0.158783 -1.201369 -0.121970  -0.063725   \n",
       "203766  403234715  28 -0.172669 -0.158783 -1.201369 -0.121970  -0.063725   \n",
       "203767  403235564  28 -0.172669 -0.158783 -1.201369 -0.121970  -0.063725   \n",
       "203768  403244581  28 -0.143292 -0.158783 -1.201369 -0.121970  -0.043875   \n",
       "\n",
       "               7         8         9  ...       158       159       160  \\\n",
       "0      -0.029140 -0.061584 -0.163591  ... -0.039637  1.461330  1.461369   \n",
       "1       0.054722 -0.061584 -0.163572  ...  0.379357  0.955101  0.459257   \n",
       "2      -0.113002 -0.061584  1.348765  ...  1.590664  0.059948  0.113967   \n",
       "3      -0.113002 -0.061584 -0.160199  ... -0.500080  0.241128  0.241406   \n",
       "4      -0.113002 -0.061584 -0.074885  ...  0.362510  0.082065  0.114773   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "203764 -0.113002 -0.061584 -0.163640  ... -0.600999  0.241128  0.241406   \n",
       "203765 -0.113002 -0.061584 -0.163323  ... -0.626229  0.241128  0.241406   \n",
       "203766 -0.113002 -0.061584 -0.163323  ... -0.626229  0.241128  0.241406   \n",
       "203767 -0.113002 -0.061584 -0.163323  ... -0.626229  0.241128  0.241406   \n",
       "203768 -0.113002 -0.061584 -0.133266  ... -0.613614  0.241128  0.241406   \n",
       "\n",
       "             161       162       163       164       165       166  class  \n",
       "0       0.018279  0.470019  1.216796  1.151607  1.519700  1.521399      2  \n",
       "1      -0.098889 -0.087490 -0.099080 -0.122137 -0.379970 -0.379288      0  \n",
       "2      -0.098889  1.969527  0.037532 -0.131010  0.006994  0.017772      0  \n",
       "3      -0.098889 -0.087490 -0.084674 -0.140597  1.519700  1.521399      2  \n",
       "4      -0.098889  8.948005  1.024948 -0.009570 -0.080708 -0.123601      2  \n",
       "...          ...       ...       ...       ...       ...       ...    ...  \n",
       "203764  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792      2  \n",
       "203765 -0.216057 -0.125939 -0.131155 -0.269818 -0.120613 -0.119792      2  \n",
       "203766 -0.216057 -0.125939 -0.131155 -0.269818 -0.120613 -0.119792      2  \n",
       "203767 -0.216057 -0.125939 -0.131155 -0.269818 -0.120613 -0.119792      2  \n",
       "203768  0.018279 -0.087490 -0.131155 -0.097524 -0.120613 -0.119792      2  \n",
       "\n",
       "[203769 rows x 168 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = df_merge.drop(['txId'], axis=1).copy()\n",
    "node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe425b9b-0193-4dd6-a2a9-87529353c375",
   "metadata": {},
   "outputs": [],
   "source": [
    "classified_idx = node_features['class'].loc[node_features['class']!=2].index\n",
    "unclassified_idx = node_features['class'].loc[node_features['class']==2].index\n",
    "\n",
    "classified_illicit_idx = node_features['class'].loc[node_features['class']==1].index\n",
    "classified_licit_idx = node_features['class'].loc[node_features['class']==0].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8282e63d-8a6c-449c-8f4b-ab8527d22aef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>...</th>\n",
       "      <th>157</th>\n",
       "      <th>158</th>\n",
       "      <th>159</th>\n",
       "      <th>160</th>\n",
       "      <th>161</th>\n",
       "      <th>162</th>\n",
       "      <th>163</th>\n",
       "      <th>164</th>\n",
       "      <th>165</th>\n",
       "      <th>166</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.168500</td>\n",
       "      <td>0.270909</td>\n",
       "      <td>-0.091383</td>\n",
       "      <td>-0.046932</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.029140</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163591</td>\n",
       "      <td>-0.164980</td>\n",
       "      <td>-0.009283</td>\n",
       "      <td>...</td>\n",
       "      <td>0.073047</td>\n",
       "      <td>-0.039637</td>\n",
       "      <td>1.461330</td>\n",
       "      <td>1.461369</td>\n",
       "      <td>0.018279</td>\n",
       "      <td>0.470019</td>\n",
       "      <td>1.216796</td>\n",
       "      <td>1.151607</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.170834</td>\n",
       "      <td>-0.131425</td>\n",
       "      <td>1.018602</td>\n",
       "      <td>0.028105</td>\n",
       "      <td>0.055376</td>\n",
       "      <td>0.054722</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.163572</td>\n",
       "      <td>-0.167757</td>\n",
       "      <td>-0.038545</td>\n",
       "      <td>...</td>\n",
       "      <td>1.228858</td>\n",
       "      <td>0.379357</td>\n",
       "      <td>0.955101</td>\n",
       "      <td>0.459257</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.099080</td>\n",
       "      <td>-0.122137</td>\n",
       "      <td>-0.379970</td>\n",
       "      <td>-0.379288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.305212</td>\n",
       "      <td>-0.210553</td>\n",
       "      <td>-1.756361</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>97.300650</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>1.348765</td>\n",
       "      <td>1.321754</td>\n",
       "      <td>-0.049707</td>\n",
       "      <td>...</td>\n",
       "      <td>1.348450</td>\n",
       "      <td>1.590664</td>\n",
       "      <td>0.059948</td>\n",
       "      <td>0.113967</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>1.969527</td>\n",
       "      <td>0.037532</td>\n",
       "      <td>-0.131010</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.017772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.169615</td>\n",
       "      <td>-0.184668</td>\n",
       "      <td>-1.201369</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>-0.043875</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.160199</td>\n",
       "      <td>-0.166062</td>\n",
       "      <td>-0.049707</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.577099</td>\n",
       "      <td>-0.500080</td>\n",
       "      <td>0.241128</td>\n",
       "      <td>0.241406</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>-0.087490</td>\n",
       "      <td>-0.084674</td>\n",
       "      <td>-0.140597</td>\n",
       "      <td>1.519700</td>\n",
       "      <td>1.521399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.086232</td>\n",
       "      <td>-0.101835</td>\n",
       "      <td>-0.646376</td>\n",
       "      <td>-0.121970</td>\n",
       "      <td>17.046997</td>\n",
       "      <td>-0.113002</td>\n",
       "      <td>-0.061584</td>\n",
       "      <td>-0.074885</td>\n",
       "      <td>-0.081943</td>\n",
       "      <td>-0.049707</td>\n",
       "      <td>...</td>\n",
       "      <td>0.501062</td>\n",
       "      <td>0.362510</td>\n",
       "      <td>0.082065</td>\n",
       "      <td>0.114773</td>\n",
       "      <td>-0.098889</td>\n",
       "      <td>8.948005</td>\n",
       "      <td>1.024948</td>\n",
       "      <td>-0.009570</td>\n",
       "      <td>-0.080708</td>\n",
       "      <td>-0.123601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 165 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        2         3         4         5          6         7         8    \\\n",
       "0 -0.168500  0.270909 -0.091383 -0.046932  -0.043875 -0.029140 -0.061584   \n",
       "1 -0.170834 -0.131425  1.018602  0.028105   0.055376  0.054722 -0.061584   \n",
       "2  1.305212 -0.210553 -1.756361 -0.121970  97.300650 -0.113002 -0.061584   \n",
       "3 -0.169615 -0.184668 -1.201369 -0.121970  -0.043875 -0.113002 -0.061584   \n",
       "4 -0.086232 -0.101835 -0.646376 -0.121970  17.046997 -0.113002 -0.061584   \n",
       "\n",
       "        9         10        11   ...       157       158       159       160  \\\n",
       "0 -0.163591 -0.164980 -0.009283  ...  0.073047 -0.039637  1.461330  1.461369   \n",
       "1 -0.163572 -0.167757 -0.038545  ...  1.228858  0.379357  0.955101  0.459257   \n",
       "2  1.348765  1.321754 -0.049707  ...  1.348450  1.590664  0.059948  0.113967   \n",
       "3 -0.160199 -0.166062 -0.049707  ... -0.577099 -0.500080  0.241128  0.241406   \n",
       "4 -0.074885 -0.081943 -0.049707  ...  0.501062  0.362510  0.082065  0.114773   \n",
       "\n",
       "        161       162       163       164       165       166  \n",
       "0  0.018279  0.470019  1.216796  1.151607  1.519700  1.521399  \n",
       "1 -0.098889 -0.087490 -0.099080 -0.122137 -0.379970 -0.379288  \n",
       "2 -0.098889  1.969527  0.037532 -0.131010  0.006994  0.017772  \n",
       "3 -0.098889 -0.087490 -0.084674 -0.140597  1.519700  1.521399  \n",
       "4 -0.098889  8.948005  1.024948 -0.009570 -0.080708 -0.123601  \n",
       "\n",
       "[5 rows x 165 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features = node_features.drop(columns=[0, 1, 'class'])\n",
    "node_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f1f77d-5374-4e4b-81ea-4340d1a4a5ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1685,  0.2709, -0.0914,  ...,  1.1516,  1.5197,  1.5214],\n",
       "        [-0.1708, -0.1314,  1.0186,  ..., -0.1221, -0.3800, -0.3793],\n",
       "        [ 1.3052, -0.2106, -1.7564,  ..., -0.1310,  0.0070,  0.0178],\n",
       "        ...,\n",
       "        [-0.1727, -0.1588, -1.2014,  ..., -0.2698, -0.1206, -0.1198],\n",
       "        [-0.1727, -0.1588, -1.2014,  ..., -0.2698, -0.1206, -0.1198],\n",
       "        [-0.1433, -0.1588, -1.2014,  ..., -0.0975, -0.1206, -0.1198]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features_t = torch.tensor(np.array(node_features.values), dtype=torch.double)\n",
    "node_features_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "38336fbf-0a79-4f48-b053-3e9962493b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([203769, 165]), (203769,))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_features_t.shape, nodes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18d0f32-ced8-4192-b187-0eb5eca2688e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bb2ab97-88e8-402e-ab5a-e61bfe2a26a3",
   "metadata": {},
   "source": [
    "# Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "86248de3-88f8-4c7f-96ad-5f0bb0a35d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "train_idx, valid_idx = train_test_split(classified_idx.values, test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03225e8c-b62f-439e-893f-d4799c7a821a",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_features_t = torch.tensor(node_features_t, dtype=torch.float32).to(device)\n",
    "edge_index = torch.tensor(edge_index, dtype=torch.long).to(device)\n",
    "weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
    "labels = torch.tensor(labels, dtype=torch.float32).to(device)\n",
    "\n",
    "data_train = Data(x=node_features_t, edge_index=edge_index, edge_attr=weights, \n",
    "                               y=torch.tensor(labels, dtype=torch.float32))\n",
    "\n",
    "data_train.train_idx = torch.tensor(train_idx, dtype=torch.long).to(device)\n",
    "data_train.valid_idx = torch.tensor(valid_idx, dtype=torch.long).to(device)\n",
    "data_train.test_idx = torch.tensor(unclassified_idx, dtype=torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3449cfc-aefd-41cc-9d67-92f4d09a11b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f24a7cf-ddf2-41ff-b04a-dde714e0ca76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d07cf0a-8e1c-46d5-8adf-35ac514e1b10",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8c600c37-f82f-48bb-97c5-82d71cce0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import torch_scatter\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch_geometric.nn as pyg_nn\n",
    "import torch_geometric.utils as pyg_utils\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import Union, Tuple, Optional\n",
    "from torch_geometric.typing import (OptPairTensor, Adj, Size, NoneType,\n",
    "                                    OptTensor)\n",
    "\n",
    "from torch.nn import Parameter, Linear\n",
    "# from torch_sparse import SparseTensor, set_diag\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "from torch_geometric.utils import remove_self_loops, add_self_loops, softmax, degree\n",
    "\n",
    "from torch_geometric.nn import GCNConv,GATConv,GATv2Conv\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80158981-583e-40df-9bd0-812eaab76750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e235a9d-a958-45ea-ae45-8af1f7d59ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricManager(object):\n",
    "    def __init__(self, modes=[\"train\", \"val\"]):\n",
    "        self.output = {}\n",
    "    \n",
    "        for mode in modes:\n",
    "          self.output[mode] = {}\n",
    "          self.output[mode][\"accuracy\"] = []\n",
    "          self.output[mode][\"f1micro\"] = []\n",
    "          self.output[mode][\"f1macro\"] = []\n",
    "          self.output[mode][\"roc_auc\"] = []\n",
    "    \n",
    "          self.output[mode][\"precision\"] = []\n",
    "          self.output[mode][\"recall\"] = []\n",
    "          self.output[mode][\"cm\"] = []\n",
    "\n",
    "    \n",
    "    def store_metrics(self, mode, pred_scores, target_labels, threshold=0.5):\n",
    "        pred_labels = pred_scores > threshold\n",
    "        accuracy = accuracy_score(target_labels, pred_labels)\n",
    "        f1micro = f1_score(target_labels, pred_labels,average='micro')\n",
    "        f1macro = f1_score(target_labels, pred_labels,average='macro')\n",
    "        aucroc = roc_auc_score(target_labels, pred_scores)\n",
    "\n",
    "        recall = recall_score(target_labels, pred_labels)\n",
    "        precision = precision_score(target_labels, pred_labels)\n",
    "        cm = confusion_matrix(target_labels, pred_labels)\n",
    "    \n",
    "        self.output[mode][\"accuracy\"].append(accuracy)\n",
    "        self.output[mode][\"f1micro\"].append(f1micro)\n",
    "        self.output[mode][\"f1macro\"].append(f1macro)\n",
    "        self.output[mode][\"roc_auc\"].append(aucroc)\n",
    "\n",
    "        self.output[mode][\"recall\"].append(recall)\n",
    "        self.output[mode][\"precision\"].append(precision)\n",
    "        self.output[mode][\"cm\"].append(cm)\n",
    "        \n",
    "        return accuracy, f1micro,f1macro, aucroc,recall,precision,cm\n",
    "      \n",
    "    def get_best(self, metric, mode=\"val\"):\n",
    "        best_results = {}\n",
    "        i = np.array(self.output[mode][metric]).argmax()\n",
    "    \n",
    "        for m in self.output[mode].keys():\n",
    "          best_results[m] = self.output[mode][m][i]\n",
    "        \n",
    "        return best_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f1917-61da-406d-912b-9dab799668ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b5f5173-54d9-4733-a450-028c1b9a3c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.metric_manager = MetricManager(modes=['train','val'])\n",
    "\n",
    "    def train(self, data_train, optimizer, loss_fun, scheduler, args):\n",
    "\n",
    "        self.data_train = data_train\n",
    "        self.model.train()\n",
    "        for epoch in range(args['epochs']):\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            y_pred = self.model(data_train).flatten()\n",
    "            loss = loss_fun(y_pred[data_train.train_idx], data_train.y[data_train.train_idx])\n",
    "\n",
    "            y_target = data_train.y[data_train.train_idx].detach().cpu().numpy()\n",
    "            pred_scores = y_pred[data_train.train_idx].detach().cpu().numpy()\n",
    "\n",
    "\n",
    "            train_acc, train_f1, train_f1macro, train_roc_auc, train_recall, train_precision, train_cm = self.metric_manager.store_metrics(\"train\", pred_scores, y_target)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Eval\n",
    "            model.eval()\n",
    "            y_target = data_train.y[data_train.valid_idx].detach().cpu().numpy()\n",
    "            pred_scores = y_pred[data_train.valid_idx].detach().cpu().numpy()\n",
    "            val_acc, val_f1,val_f1macro, val_roc_auc, val_recall, val_precision, val_cm = self.metric_manager.store_metrics(\"val\", pred_scores, y_target)\n",
    "\n",
    "            if epoch % 50 == 0:\n",
    "                print(\"epoch: {} - loss: {:.4f} - accuracy train: {:.4f} -accuracy valid: {:.4f}  - val roc: {:.4f}  - val f1micro: {:.4f}\".format(epoch, loss.item(), train_acc, val_acc, val_roc_auc, val_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f6c81e-ad4d-410e-be20-c6e06eb260a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88a5700-7df6-4a3d-b897-040b14295da9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274475c-837d-4f5c-baae-714a119ad990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e635ec6a-ef01-43b5-9b37-6fd13b8f61d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9366ac2-35e2-4478-93cf-eeb85378b14f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d943e92a-2846-4cd1-b63f-7fca992e9021",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5261614c-93a2-477e-b608-58053596667e",
   "metadata": {},
   "source": [
    "# GAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "452692f0-3d31-4425-a609-04c59a5c2f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Static update weigths\n",
    "\n",
    "class GAT(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 hidden_dim, \n",
    "                 out_channels, \n",
    "                 heads, \n",
    "                 args\n",
    "    ):\n",
    "        super(GAT, self).__init__()\n",
    "\n",
    "        self.dropout = args.get('dropout') if args else 0.5\n",
    "        self.training_flag = args.get('training', True) if args else True\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            pyg_nn.GATConv(in_channels=in_channels, out_channels=hidden_dim, heads=heads),\n",
    "            pyg_nn.GATConv(in_channels=hidden_dim * heads, out_channels=hidden_dim, heads=heads),\n",
    "        ])\n",
    "\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim * heads, hidden_dim),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, out_channels)\n",
    "        )\n",
    "    \n",
    "    def forward(self, data, adj=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for gat in self.layers:\n",
    "            x = gat(x, edge_index)\n",
    "            x = F.dropout(F.relu(x), p=self.dropout, training=self.training_flag)\n",
    "\n",
    "        x = self.mlp(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624a918-5e19-4ff2-9b79-67745caab981",
   "metadata": {},
   "source": [
    "# GATv2Conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "5f2f4776-41f1-429c-8864-f82b7d6e1e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GATv2(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels, args):\n",
    "        super(GATv2, self).__init__()\n",
    "\n",
    "        heads = args.get('heads')\n",
    "        \n",
    "        self.dropout = args.get('dropout') \n",
    "        self.training_flag = args.get('training', True) if args else True\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            pyg_nn.GATv2Conv(in_channels=in_channels, out_channels=hidden_dim, heads=heads),\n",
    "            pyg_nn.GATv2Conv(in_channels=hidden_dim * heads, out_channels=hidden_dim, heads=heads),\n",
    "        ])  \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(heads * hidden_dim, hidden_dim),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(hidden_dim, out_channels)    \n",
    "        )\n",
    "\n",
    "    def forward(self, data, adj=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = F.dropout(F.relu(x), p=self.dropout, training=self.training_flag)\n",
    "\n",
    "        x = self.mlp(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2687c1-666a-41e7-b721-17cfcfc4112e",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "207b537e-9d92-497f-bf93-9f78af2dcf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, \n",
    "                 in_channels, \n",
    "                 hidden_dim,\n",
    "                 out_channels, \n",
    "                 args                 \n",
    "    ):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(22)\n",
    "\n",
    "        heads = args['heads']\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            pyg_nn.GCNConv(in_channels=in_channels, out_channels=hidden_dim, normalize=True),\n",
    "            pyg_nn.GCNConv(in_channels=hidden_dim, out_channels=out_channels, normalize=True)\n",
    "        ])\n",
    "        self.mlp = nn.Linear(out_channels, 1)\n",
    "\n",
    "    def forward(self, data, adj=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = x.tanh()\n",
    "\n",
    "        x = self.mlp(x)        \n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8202b316-c88a-4f41-bea6-0a9968527ea7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85063f93-2d63-45e4-a015-6b0dd6222626",
   "metadata": {},
   "source": [
    "# GraphSAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cdef236-2734-4ba6-9c65-0869cc9ea29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels, num_layers, args):\n",
    "        super(GraphSAGE, self).__init__()\n",
    "\n",
    "        self.dropout = args['dropout']        \n",
    "        self.layers = nn.ModuleList([\n",
    "            pyg_nn.GraphSAGE(in_channels=in_channels, hidden_channels=hidden_dim, out_channels=out_channels, num_layers=num_layers),\n",
    "            pyg_nn.GraphSAGE(in_channels=in_channels, hidden_channels=hidden_dim, out_channels=out_channels, num_layers=num_layers),\n",
    "        ])\n",
    "        \n",
    "        self.linear = nn.Linear(out_channels, in_channels)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_channels, 2 * in_channels),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(2 * in_channels, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, data, adj=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = self.linear(x)\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c20da2b-b714-4ce9-8e2e-0e6d543fddbd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a884a712-5d98-4916-a5a4-61cfda8d5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GIN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dim, out_channels):\n",
    "        super(GIN, self).__init__()\n",
    "        \n",
    "        nn1 = nn.Sequential(\n",
    "            nn.Linear(in_channels, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim)\n",
    "        )\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            pyg_nn.GINConv(nn1, eps=0.1),\n",
    "            pyg_nn.GINConv(nn1, eps=0.5),\n",
    "        ])\n",
    "        \n",
    "        self.additional_layer = nn.Linear(hidden_dim, in_channels)\n",
    "        self.mlp = nn.Linear(hidden_dim, out_channels)\n",
    "        \n",
    "\n",
    "    def forward(self, data, adj=None):\n",
    "        x, edge_index = data.x, data.edge_index\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = self.additional_layer(x)\n",
    "        \n",
    "        x = self.mlp(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95044ac3-d817-441a-a746-ad1ae895b4e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceed75d7-4348-4cec-bce3-e10dcd198d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd775eec-57e4-491b-8ebe-238a60e6ae3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4bab0-b73e-4728-919b-0aae75cc7910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920044be-d84e-43b7-9bf6-493eef9a46fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae31248-c94c-475e-b71e-a953589b20b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90eaf560-4c95-487f-a94a-54ead3b9bd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcd0db53-432c-47ba-a96b-7621bf917a2e",
   "metadata": {},
   "source": [
    "# Train GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "07f54193-9231-4a07-ba40-11cda9a7dad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "args = {\n",
    "    \"epochs\":101,\n",
    "    'lr':0.01,\n",
    "    'weight_decay':1e-5,\n",
    "    'prebuild':True,\n",
    "    'heads': 4,\n",
    "    'training': True,\n",
    "    'hidden_dim': 128, \n",
    "    'dropout': 0.5\n",
    "}\n",
    "\n",
    "in_channels = data_train.x.shape[1]\n",
    "hidden_dim = args['hidden_dim']\n",
    "out_channels = 1\n",
    "heads = args['heads']\n",
    "\n",
    "\n",
    "model = GCN(in_channels=in_channels, hidden_dim=hidden_dim, out_channels=out_channels, args=args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "model = model.to(device)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=10, \n",
    "    num_training_steps=50\n",
    ")\n",
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "fb4f438d-d8cd-4df5-a58c-b8b6334c48e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - loss: 0.4128 - accuracy train: 0.9028 -accuracy valid: 0.9004  - val roc: 0.5338  - val f1micro: 0.9004\n",
      "epoch: 50 - loss: 0.3233 - accuracy train: 0.9028 -accuracy valid: 0.9004  - val roc: 0.9145  - val f1micro: 0.9004\n",
      "epoch: 100 - loss: 0.2546 - accuracy train: 0.9028 -accuracy valid: 0.9004  - val roc: 0.9505  - val f1micro: 0.9004\n"
     ]
    }
   ],
   "source": [
    "gсn_trainer = Trainer(model)\n",
    "gсn_trainer.train(data_train, optimizer, loss_fun, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2142e3-849b-46f2-b6e1-fc7809fb50b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "fbda991d-33b1-42ec-b66c-031493ab520b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "args = {\n",
    "    \"epochs\":101,\n",
    "    'lr':0.01,\n",
    "    'weight_decay':1e-5,\n",
    "    'prebuild':True,\n",
    "    'heads': 4,\n",
    "    'training': True,\n",
    "    'hidden_dim': 128, \n",
    "    'dropout': 0.5\n",
    "}\n",
    "\n",
    "in_channels = data_train.x.shape[1]\n",
    "hidden_dim = args['hidden_dim']\n",
    "out_channels = 1\n",
    "heads = args['heads']\n",
    "\n",
    "model = GATv2(in_channels=in_channels, hidden_dim=hidden_dim, out_channels=out_channels, args=args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "model = model.to(device)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=10, \n",
    "    num_training_steps=50\n",
    ")\n",
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "6e46548e-aead-4a7d-a890-58d3eb00d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - loss: 0.7324 - accuracy train: 0.4772 -accuracy valid: 0.4733  - val roc: 0.5359  - val f1micro: 0.4733\n",
      "epoch: 50 - loss: 0.1335 - accuracy train: 0.9603 -accuracy valid: 0.9569  - val roc: 0.9707  - val f1micro: 0.9569\n",
      "epoch: 100 - loss: 0.0541 - accuracy train: 0.9833 -accuracy valid: 0.9772  - val roc: 0.9846  - val f1micro: 0.9772\n"
     ]
    }
   ],
   "source": [
    "gсn_trainer = Trainer(model)\n",
    "gсn_trainer.train(data_train, optimizer, loss_fun, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed2be52-146c-4728-b23f-9ab93eed9ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "41bcb496-ee22-4685-86d1-584fba7008a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "args = {\n",
    "    \"epochs\":101,\n",
    "    'lr':0.01,\n",
    "    'weight_decay':1e-5,\n",
    "    'prebuild':True,\n",
    "    'heads': 4,\n",
    "    'training': True,\n",
    "    'hidden_dim': 128, \n",
    "    'dropout': 0.5\n",
    "}\n",
    "\n",
    "in_channels = data_train.x.shape[1]\n",
    "hidden_dim = args['hidden_dim']\n",
    "out_channels = 1\n",
    "heads = args['heads']\n",
    "\n",
    "model = GAT(in_channels=in_channels, hidden_dim=hidden_dim, out_channels=out_channels,heads=heads, args=args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "model = model.to(device)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=10, \n",
    "    num_training_steps=50\n",
    ")\n",
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "d201ede6-6c97-4063-9b1a-57c12ac7832d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - loss: 0.8132 - accuracy train: 0.3457 -accuracy valid: 0.3591  - val roc: 0.4697  - val f1micro: 0.3591\n",
      "epoch: 50 - loss: 0.1470 - accuracy train: 0.9509 -accuracy valid: 0.9480  - val roc: 0.9659  - val f1micro: 0.9480\n",
      "epoch: 100 - loss: 0.0758 - accuracy train: 0.9764 -accuracy valid: 0.9709  - val roc: 0.9839  - val f1micro: 0.9709\n"
     ]
    }
   ],
   "source": [
    "gсn_trainer = Trainer(model)\n",
    "gсn_trainer.train(data_train, optimizer, loss_fun, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebd1cab-183e-4e71-b856-c054c32afd41",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "35a5d07a-3d3f-47c5-857b-27f871b753df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "args = {\n",
    "    \"epochs\":101,\n",
    "    'lr':0.01,\n",
    "    'weight_decay':1e-5,\n",
    "    'prebuild':True,\n",
    "    'heads': 4,\n",
    "    'training': True,\n",
    "    'hidden_dim': 128, \n",
    "    'dropout': 0.5\n",
    "}\n",
    "\n",
    "in_channels = data_train.x.shape[1]\n",
    "hidden_dim = args['hidden_dim']\n",
    "out_channels = 1\n",
    "heads = args['heads']\n",
    "\n",
    "model = GraphSAGE(in_channels=in_channels, hidden_dim=hidden_dim, out_channels=out_channels, num_layers=2, args=args)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "model = model.to(device)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=10, \n",
    "    num_training_steps=50\n",
    ")\n",
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e13e71b6-38e4-4aaf-a8f3-3000e9caeed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 - loss: 0.8178 - accuracy train: 0.1533 -accuracy valid: 0.1449  - val roc: 0.4999  - val f1micro: 0.1449\n",
      "epoch: 50 - loss: 0.1828 - accuracy train: 0.9332 -accuracy valid: 0.9346  - val roc: 0.9233  - val f1micro: 0.9346\n",
      "epoch: 100 - loss: 0.0693 - accuracy train: 0.9776 -accuracy valid: 0.9696  - val roc: 0.9791  - val f1micro: 0.9696\n"
     ]
    }
   ],
   "source": [
    "gсn_trainer = Trainer(model)\n",
    "gсn_trainer.train(data_train, optimizer, loss_fun, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ec2aa-d253-4264-85a1-be944e4a3114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5d56d8a3-3855-4c28-8baf-10549dfd5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "args = {\n",
    "    \"epochs\":101,\n",
    "    'lr':0.01,\n",
    "    'weight_decay':1e-5,\n",
    "    'prebuild':True,\n",
    "    'heads': 4,\n",
    "    'training': True,\n",
    "    'hidden_dim': 128, \n",
    "    'dropout': 0.5\n",
    "}\n",
    "\n",
    "in_channels = data_train.x.shape[1]\n",
    "hidden_dim = args['hidden_dim']\n",
    "out_channels = 1\n",
    "heads = args['heads']\n",
    "\n",
    "model = GIN(in_channels=in_channels, hidden_dim=hidden_dim, out_channels=out_channels)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'], weight_decay=args['weight_decay'])\n",
    "model = model.to(device)\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=10, \n",
    "    num_training_steps=50\n",
    ")\n",
    "loss_fun = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3b2f8a9a-805d-4bac-8292-c7c5cc250dbc",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (203769x128 and 165x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m gсn_trainer \u001b[38;5;241m=\u001b[39m Trainer(model)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mgсn_trainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, data_train, optimizer, loss_fun, scheduler, args)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]):\n\u001b[1;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 14\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     15\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fun(y_pred[data_train\u001b[38;5;241m.\u001b[39mtrain_idx], data_train\u001b[38;5;241m.\u001b[39my[data_train\u001b[38;5;241m.\u001b[39mtrain_idx])\n\u001b[1;32m     17\u001b[0m     y_target \u001b[38;5;241m=\u001b[39m data_train\u001b[38;5;241m.\u001b[39my[data_train\u001b[38;5;241m.\u001b[39mtrain_idx]\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[43], line 22\u001b[0m, in \u001b[0;36mGIN.forward\u001b[0;34m(self, data, adj)\u001b[0m\n\u001b[1;32m     19\u001b[0m x, edge_index \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mx, data\u001b[38;5;241m.\u001b[39medge_index\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 22\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(x)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch_geometric/nn/conv/gin_conv.py:90\u001b[0m, in \u001b[0;36mGINConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     88\u001b[0m     out \u001b[38;5;241m=\u001b[39m out \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps) \u001b[38;5;241m*\u001b[39m x_r\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/mnt/tank/scratch/rgurtsiev/miniconda3/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (203769x128 and 165x128)"
     ]
    }
   ],
   "source": [
    "gсn_trainer = Trainer(model)\n",
    "gсn_trainer.train(data_train, optimizer, loss_fun, scheduler, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8667e7f7-ffb8-47fe-8b45-57e1e224d582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9b400-7cd4-4ab3-b891-6d6a36882561",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fec5a2-1744-4dfc-93bc-2fa0e111f711",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fe0de2-6b6b-4016-829b-fa32c7d27cae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c09990-f14d-44f6-a183-841c93c2f000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01439020-7002-45ad-aefb-b18602821999",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b266ea-ccf2-4458-8524-2986901f3a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c9981-691f-49be-9214-6ec5a05bae88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405444f7-7197-4245-819e-020599056739",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5190de1-6c4e-41d6-a910-8da5dffec16c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a606bf8-4bf3-45ed-aa14-6fe8c820755d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2eb172-0d61-4d26-b540-e8a3c2dc37fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7e873a-e247-48ee-ba29-4904992b717c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5f032f-63e9-463c-8038-35dfe5df9dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545d4ee-1ae4-4e72-932c-5f5e6c25c101",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d4035-3558-4139-a4a7-7fd31cbd33e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a348d8df-5669-4296-b872-85bcfff1c5ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ddb31b-dbcf-41a6-98e7-9a0fb82b6a5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0401e26f-479e-4098-a954-06b5d149d794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db23defe-8807-43ce-a083-2b31bac20dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
